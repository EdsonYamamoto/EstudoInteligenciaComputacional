{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redução de Dimensionalidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn import datasets\n",
    "\n",
    "wine = datasets.load_wine()\n",
    "X = wine.data\n",
    "y = wine.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(X)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print('Acurácia nos dados originais:', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "print('Número original de atributos:', X_train.shape[1])\n",
    "print('Número reduzido de atributos:', X_test_pca.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "model.fit(X_train_pca, y_train)\n",
    "y_pred = model.predict(X_test_pca)\n",
    "\n",
    "print('Acurácia nos dados originais:', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# procurando quantidade de atributos ideais\n",
    "for n in range(1,14):\n",
    "\n",
    "    pca = PCA(n_components=n)\n",
    "\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    \n",
    "    model = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "    model.fit(X_train_pca, y_train)\n",
    "    y_pred = model.predict(X_test_pca)\n",
    "\n",
    "    print('Components: ', n, '- Acurácia nos dados originais:', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seleção de atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "fvalue_selector = SelectKBest(f_classif, k=4)\n",
    "X_train_kbest = fvalue_selector.fit_transform(X_train, y_train)\n",
    "X_test_kbest = fvalue_selector.transform(X_test)\n",
    "\n",
    "print('Número original de atributos:', X_train.shape[1])\n",
    "print('Número reduzido de atributos:', X_train_kbest.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print('Acurácia nos dados originais:', accuracy_score(y_test, y_pred))\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=1)\n",
    "model.fit(X_train_kbest, y_train)\n",
    "y_pred = model.predict(X_test_kbest)\n",
    "print('Acurácia nos dados (Kbest):', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# procurando quantidade de atributos ideais\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print('Acurácia nos dados originais:', accuracy_score(y_test, y_pred))\n",
    "\n",
    "baseline = accuracy_score(y_test, y_pred)\n",
    "\n",
    "kbest = []\n",
    "acc   = []\n",
    "\n",
    "for n in range(1,14):\n",
    "    \n",
    "    fvalue_selector = SelectKBest(f_classif, k=n)\n",
    "    X_train_kbest = fvalue_selector.fit_transform(X_train, y_train)\n",
    "    X_test_kbest = fvalue_selector.transform(X_test)\n",
    "    \n",
    "    model = KNeighborsClassifier(n_neighbors=1)\n",
    "    model.fit(X_train_kbest, y_train)\n",
    "    y_pred = model.predict(X_test_kbest)\n",
    "    \n",
    "    kbest.append(n)\n",
    "    acc.append(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    print('Features:', n, '- Acurácia nos dados (Kbest):', acc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "grid = sns.JointGrid(kbest, acc, space=0, size=6, ratio=50)\n",
    "grid.plot_joint(plt.bar, color=\"g\")\n",
    "plt.plot([min(kbest), max(kbest)], [baseline, baseline], linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import dump, load\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=1)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "persistence = {}\n",
    "persistence['scaler'] = scaler\n",
    "persistence['model']  = model\n",
    "\n",
    "dump(persistence, 'persist.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persistence = load('persist.joblib')\n",
    "\n",
    "scaler = persistence['scaler']\n",
    "model = persistence['model']\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "y_pred = model.predict(X_test_kbest)\n",
    "\n",
    "print('Acurácia:', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agrupamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construção de alguns conjuntos artificiais\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "np.random.seed(844)\n",
    "clust1 = np.random.normal(5, 2, (1000,2))\n",
    "clust2 = np.random.normal(15, 3, (1000,2))\n",
    "clust3 = np.random.multivariate_normal([17,3], [[1,0],[0,1]], 1000)\n",
    "clust4 = np.random.multivariate_normal([2,16], [[1,0],[0,1]], 1000)\n",
    "dataset1 = np.concatenate((clust1, clust2, clust3, clust4))\n",
    "\n",
    "# we take the first array as the second array has the cluster labels\n",
    "dataset2 = datasets.make_circles(n_samples=1000, factor=.5, noise=.05)[0]\n",
    "\n",
    "# plot clustering output on the two datasets\n",
    "def cluster_plots(set1, set2, colours1 = 'gray', colours2 = 'gray', \n",
    "                  title1 = 'Dataset 1',  title2 = 'Dataset 2'):\n",
    "    fig,(ax1,ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(6, 3)\n",
    "    ax1.set_title(title1,fontsize=14)\n",
    "    ax1.set_xlim(min(set1[:,0]), max(set1[:,0]))\n",
    "    ax1.set_ylim(min(set1[:,1]), max(set1[:,1]))\n",
    "    ax1.scatter(set1[:, 0], set1[:, 1],s=8,lw=0,c= colours1)\n",
    "    ax2.set_title(title2,fontsize=14)\n",
    "    ax2.set_xlim(min(set2[:,0]), max(set2[:,0]))\n",
    "    ax2.set_ylim(min(set2[:,1]), max(set2[:,1]))\n",
    "    ax2.scatter(set2[:, 0], set2[:, 1],s=8,lw=0,c=colours2)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "cluster_plots(dataset1, dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agrupamento por k-medias\n",
    "kmeans_dataset1 = cluster.KMeans(n_clusters=4, max_iter=300, \n",
    "                                 init='k-means++',n_init=10).fit_predict(dataset1)\n",
    "kmeans_dataset2 = cluster.KMeans(n_clusters=2, max_iter=300, \n",
    "                                 init='k-means++',n_init=10).fit_predict(dataset2)\n",
    "print('Dataset1')\n",
    "print(*[\"Cluster \"+str(i)+\": \"+ str(sum(kmeans_dataset1==i)) for i in range(4)], sep='\\n')\n",
    "cluster_plots(dataset1, dataset2, kmeans_dataset1, kmeans_dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensível a densidade, o que acontece em diminuir as amostras?\n",
    "\n",
    "kmeans_dataset3 = cluster.KMeans(n_clusters=4, max_iter=300, \n",
    "                                 init='k-means++',n_init=10).fit_predict(np.vstack([dataset1[:2080,:],\n",
    "                                                                                    dataset1[3000:3080,:]]))\n",
    "kmeans_dataset4 = cluster.KMeans(n_clusters=4, max_iter=300, \n",
    "                                 init='k-means++',n_init=10).fit_predict(np.vstack([dataset1[-2080:,],\n",
    "                                                                                    dataset1[:80,]]))\n",
    "cluster_plots(np.vstack([dataset1[:2080,],dataset1[3000:3080,]]), \n",
    "              np.vstack([dataset1[-2080:,],dataset1[:80,]]), \n",
    "              kmeans_dataset3, kmeans_dataset4,title1='', title2='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_dataset1 = cluster.AgglomerativeClustering(n_clusters=4, affinity='euclidean', \n",
    "                                              linkage='ward').fit_predict(dataset1)\n",
    "hc_dataset2 = cluster.AgglomerativeClustering(n_clusters=2, affinity='euclidean', \n",
    "                                              linkage='average').fit_predict(dataset2)\n",
    "print(\"Dataset 1\")\n",
    "print(*[\"Cluster \"+str(i)+\": \"+ str(sum(hc_dataset1==i)) for i in range(4)], sep='\\n')\n",
    "cluster_plots(dataset1, dataset2, hc_dataset1, hc_dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_dataset3 = cluster.AgglomerativeClustering(n_clusters=2, affinity='euclidean', \n",
    "                                              linkage='complete').fit_predict(dataset2)\n",
    "connect = kneighbors_graph(dataset2, n_neighbors=5, include_self=False)\n",
    "hc_dataset3_connectivity = cluster.AgglomerativeClustering(n_clusters=2, affinity='euclidean', \n",
    "                                              linkage='complete',connectivity=connect).fit_predict(dataset2)\n",
    "cluster_plots(dataset2, dataset2, hc_dataset3, hc_dataset3_connectivity,\n",
    "             title1='Without Connectivity', title2='With Connectivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classificacao com KMeans\n",
    "\n",
    "from sklearn.datasets import load_digits, load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def clust(dataset_load, n_clusters):\n",
    "\n",
    "    ds = dataset_load()\n",
    "    X = ds.data\n",
    "    y = ds.target\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2019)\n",
    "\n",
    "    model = KNeighborsClassifier(n_neighbors=1)\n",
    "    model.fit(X_train, y_train)\n",
    "    print('Sem clustering:', model.score(X_test, y_test))\n",
    "\n",
    "    km = KMeans(n_clusters = n_clusters, random_state=42)\n",
    "    X_train = km.fit_transform(X_train)\n",
    "    X_test  = km.transform(X_test)\n",
    "\n",
    "    model = KNeighborsClassifier(n_neighbors=1)\n",
    "    model.fit(X_train, y_train)\n",
    "    print('Com clustering:', model.score(X_test, y_test))\n",
    "    \n",
    "\n",
    "print('Digits dataset')\n",
    "clust(load_digits, n_clusters = 3)\n",
    "print()\n",
    "print('Iris dataset')\n",
    "clust(load_iris, n_clusters = 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercícios\n",
    "\n",
    "1. Explore a utilização do PCA e do SelectKBest dentro do Pipeline para simplificar a aplicação."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
